{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence\n",
    "\n",
    "## Inputs\n",
    "Korean letter level (가-힣)  \n",
    "(Only the characters in the dataset)\n",
    "\n",
    "조합된 글자 자체를 출력하되, 데이터 셋에 있는 글자만 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready for data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special letter\n",
    "space = ' '\n",
    "special_letters = [space]\n",
    "\n",
    "\n",
    "# English data\n",
    "\n",
    "big = [chr(i) for i in range(ord('A'), ord('Z')+1)]\n",
    "small = [chr(i) for i in range(ord('a'), ord('z')+1)]\n",
    "\n",
    "english_letter_list = big + small\n",
    "\n",
    "# Add special letters\n",
    "english_letter_list.extend(special_letters)\n",
    "\n",
    "# Make index mapping\n",
    "idx2english = dict(enumerate(english_letter_list, 1))\n",
    "english2idx = {v: k for k, v in idx2english.items()}\n",
    "\n",
    "\n",
    "# Read dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/demo_data.csv', encoding='utf8')\n",
    "del df['Unnamed: 0']\n",
    "\n",
    "\n",
    "# Korean data\n",
    "hang = df['hang'].values\n",
    "\n",
    "korean_letter_list = list(set([letter for sent in hang for letter in sent]))\n",
    "\n",
    "# Make index mapping\n",
    "idx2korean = dict(enumerate(korean_letter_list, 1))\n",
    "korean2idx = {v: k for k, v in idx2korean.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 38.  27.  46. ...,   0.   0.   0.]\n",
      " [ 27.  42.  42. ...,   0.   0.   0.]\n",
      " [ 30.  31.  29. ...,   0.   0.   0.]\n",
      " ..., \n",
      " [ 27.  35.  44. ...,   0.   0.   0.]\n",
      " [ 27.  35.  44. ...,   0.   0.   0.]\n",
      " [ 27.  35.  44. ...,   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "# Load X\n",
    "\n",
    "roma = df['roma'].apply(lambda x: np.array([english2idx[letter] for letter in x]))\n",
    "roma_df = pd.DataFrame(roma)\n",
    "X = pd.DataFrame(roma_df['roma'].tolist()).values\n",
    "\n",
    "# Replace nan to 0 \n",
    "# Get index of nan, and make them 0\n",
    "X[np.isnan(X)] = 0\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  40.  179.  144. ...,    0.    0.    0.]\n",
      " [ 191.   21.    0. ...,    0.    0.    0.]\n",
      " [ 153.  214.  134. ...,    0.    0.    0.]\n",
      " ..., \n",
      " [  91.  136.  114. ...,    0.    0.    0.]\n",
      " [  91.  136.   23. ...,    0.    0.    0.]\n",
      " [  91.  136.   23. ...,    0.    0.    0.]]\n",
      "(300, 12)\n",
      "(300, 12, 218)\n"
     ]
    }
   ],
   "source": [
    "# Load y\n",
    "\n",
    "hang = df['hang'].apply(lambda x: np.array([korean2idx[letter] for letter in x]))\n",
    "hang_df = pd.DataFrame(hang)\n",
    "y = pd.DataFrame(hang_df['hang'].tolist()).values\n",
    "\n",
    "# Replace nan to 0\n",
    "# Get index of nan, and make them 0\n",
    "y[np.isnan(y)] = 0\n",
    "print(y)\n",
    "print(y.shape)\n",
    "\n",
    "# one hot encoding\n",
    "from keras.utils import np_utils\n",
    "y = np_utils.to_categorical(y, num_classes=len(korean_letter_list)+1).reshape(300, 12, len(korean_letter_list)+1)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train & test & val set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import recurrent\n",
    "\n",
    "# Parameters for the model and dataset\n",
    "TRAINING_SIZE = 50000\n",
    "VOCAB_SIZE = 12\n",
    "INVERT = True\n",
    "HIDDEN_SIZE = 200\n",
    "BATCH_SIZE = 100\n",
    "LAYERS = 2\n",
    "MAX_EPOCHS = 1000\n",
    "EMBEDDING_OUTPUT_SIZE = 128\n",
    "MAX_SENT_LENGTH = X.shape[1] + 1\n",
    "\n",
    "RNN = recurrent.GRU\n",
    "stop_monitor = 'val_acc'\n",
    "stop_delta = 0.0\n",
    "stop_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, TimeDistributed, Activation, RepeatVector, Embedding, Dropout\n",
    "\n",
    "def build_model(\n",
    "    embedding_input_dim,\n",
    "    embedding_output_dim,\n",
    "    embedding_input_length,\n",
    "    hidden_size,\n",
    "    maximum_output_length,\n",
    "    encoder_layer_count,\n",
    "    dropout_rate,\n",
    "    output_classes_count,\n",
    "    rnn_layer,\n",
    "    summary_type=False):\n",
    "    \n",
    "    print('Build Model...')\n",
    "    \n",
    "    RNN = rnn_layer\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(\n",
    "        input_dim=embedding_input_dim,\n",
    "        output_dim=embedding_output_dim,\n",
    "        input_length=embedding_input_length\n",
    "    ))\n",
    "    \n",
    "    model.add(RNN(hidden_size, return_sequences=True, input_shape=(maximum_input_length, )))\n",
    "    model.add(RNN(hidden_size))\n",
    "    \n",
    "    model.add(RepeatVector(maximum_output_length))  # Maximum output length\n",
    "    \n",
    "    for _ in range(encoder_layer_count):\n",
    "        model.add(RNN(hidden_size, return_sequences=True))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        \n",
    "    model.add(TimeDistributed(Dense(output_classes_count)))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    if summary_type is False:\n",
    "        pass\n",
    "    elif str(summary_type).lower() == 'svg':\n",
    "        viz_model(model)\n",
    "    else:\n",
    "        model.summary()\n",
    "        \n",
    "    return model\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
